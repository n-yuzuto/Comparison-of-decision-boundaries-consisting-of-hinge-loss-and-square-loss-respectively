# Comparison-of-decision-boundaries-consisting-of-hinge-loss-and-square-loss-respectively
ランダムに生成された40点の2クラス分類問題のための訓練事例について、ヒンジ損失と二乗損失で線形分類モデルを学習し、その決定境界を表示している。前者は外れ値を含まない訓練事例、後者は外れ値を含む訓練事例で学習させた結果である。
それぞれ分類のために分類のための超平面を学習させている。

【損失関数の性質】  
 二乗損失：
  - 目標値と差の二乗、回帰に利用
  - 凸関数、微分可能
  - 総損失䛾最小化
    - 解析的に求まる,	厳密な最適解
    - 最急降下法でも解ける、近似的な解

ヒンジ損失:
- 超平面からの距離(正しく分類されない側にどれだけ離れているか)
- 凸関数、ただし微分不可能
- 総損失の最小化
  - QP （ソフトマージンSVMもQPで解ける）,	厳密な最適解
  - 劣勾配降下法,	近似的な解

【考察】  
外れ値を含まない場合：  
 - それぞれ学習された線形モデルが為す超平面の次元数は、一致する
 - それどれ学習された線形モデルは、見た目は同様に見えるが、実際には同じ線形モデルにはならない
 - どちらの場合も目的関数は凸関数である
 
外れ値を含む場合:  
 - 訓練サンプルが外れ値を含んでいても、多項式特徴量は利用できる
 - 訓練サンプルが外れ値を含む場合、二乗損失で学習したモデルは適切に学習されない可能性が高い
 
【まとめ】  
外れ値を含む場合、学習されたモデルの挙動が異なる。これは、ti(wxi)T<0における二乗損失が正の値を取るためである。  
二乗損失はヒンジ損失よりも損失に大きな影響を与えてしまう。





***
.ipynbファイルが開かれない時は、こちらのリンクにURLを貼ってご覧になってください。  
[nbviewer](https://nbviewer.jupyter.org/)
